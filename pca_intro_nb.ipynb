{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c77ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef403b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <i class=\"fas fa-circle\" style=\"color: #f0ad4e;\"></i> Introduction to PCA\n",
    "\n",
    "In this exercise you will create a basic PCA implementation that you will use for the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b73c3",
   "metadata": {},
   "source": [
    "## Implementing PCA\n",
    "\n",
    "Your first task is to implement the PCA method as well as functions for\n",
    "transforming to and from the space defined by the principal components.\n",
    "But first, a quick recap of the terminology to minimize confusion.\n",
    "\n",
    "Principal component analysis is about finding a linear transformation\n",
    "that reduces the number of dimensions used to represent samples while\n",
    "destroying as little of the variation as possible. PCA is defined by\n",
    "$\\Phi_{:k}$, an $M\\times k$ matrix representing a linear transformation from\n",
    "vectors in $M$-dimensional real space to $k$-dimensional latent space. We have the\n",
    "following transformations\n",
    "\n",
    "$$ \n",
    "b = \\Phi_{:k}^\\top x, \n",
    "$$\n",
    "\n",
    "$$\n",
    "x + \\epsilon = \\Phi_{:k} b,\n",
    "$$\n",
    "\n",
    "where\n",
    "$x\\in\\mathbb{R}^M$ is the input vector and $y\\in\\mathbb{R}^K$ is the\n",
    "embedded vector. As shown in the second equation, it is possible to reconstruct $x$ with some amount of error $\\epsilon$. To find $\\Phi$, we use the _eigenvectors_ of the covariance matrix of our data matrix $W$ where each row $i$ is a sample $x_i \\in \\mathbb{R}^M$. The eigenvectors are sorted by their associated eigenvalues which represent the variance of each dimension in latent space. Selecting the $k$ first columns (we use the notation $\\Phi_{:k}$) results in a transformation that reduces the dimensionality of the latent space to $k$ dimensions. \n",
    "\n",
    "1.  <i class=\"fas fa-code\"></i> **Setup:** Create a script file or notebook for this exercise. In it, start by loading the face shapes and images using `dset.face_shapes_data`.\n",
    "\n",
    "2.  <i class=\"fas fa-code\"></i> **Implement PCA:** Create a function that calculates and returns the\n",
    "    principle components of the shapes dataset. Use the method described\n",
    "    above where the eigenvectors of the covariance matrix is used.\n",
    "    **Make sure to center the samples (subtract the mean before\n",
    "    calculating the covariance matrix)**.\n",
    "\n",
    "3.  <i class=\"fas fa-code\"></i> **Implement transformations:** Create two functions, one for\n",
    "    transforming from feature space to principal component space\n",
    "    (eqaution {eq}`trans`) and one for transforming from principal\n",
    "    component space to feature space\n",
    "    (equation {eq}`inv`). You have to subtract the $\\mu$ vector when\n",
    "    transforming to the principal component space and add it again when\n",
    "    transforming back to feature space. You may use the following\n",
    "    modified equations for reference:\n",
    "\n",
    "$$\n",
    "b = \\Phi_{:k}^\\top(x-\\mu)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "x = \\Phi_{:k} b + \\mu\n",
    "$$\n",
    "\n",
    "```{tip}\n",
    "The reading material for the PCA lecture contains an excellent [tutorial](https://sebastianraschka.com/Articles/2014_pca_step_by_step.html) on how this can be done, but remember that copying is not allowed!!\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "Some of the later tasks will be easier if you return all 146 principle components. You can then create another function for extracting $n$ components to generate $\\Phi$.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57776428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "(120, 146)\n",
      "(146, 120)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "path = './db'\n",
    "shapes, images = face_shape_data(path)\n",
    "print(shapes.shape)\n",
    "shapes = shapes.T\n",
    "print(shapes.shape) # in pca each row represent a person so we got 146 entites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e95e624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your implementation here.\n",
    "#print(shapes[0])\n",
    "# PCA\n",
    "def mean(X):\n",
    "    mean_vector = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        tmp_mean_vector = np.mean(shapes[i, :])\n",
    "        mean_vector.append(tmp_mean_vector)\n",
    "\n",
    "    mean_vector = np.array(mean_vector)\n",
    "   \n",
    "    return mean_vector\n",
    "# print(mean(shapes))\n",
    "    \n",
    "def eigns(X,k):\n",
    "    n,m = np.shape(X)\n",
    "    \n",
    "    # Calculating the mean of the shapes dataset\n",
    "    #mean_x = np.mean(X)\n",
    "    \n",
    "    # Representing the mean to correspond the shapes dataset\n",
    "    #y = np.ones((n, m), dtype=int)\n",
    "    #mean_of_x = np.dot(mean_x,y)\n",
    "    #print(mean_x)\n",
    "    \n",
    "    mean_vector = mean(X)\n",
    "    mean_ = np.array([mean_vector] * 120)\n",
    "    # Subtract the mean from the shapes dataset\n",
    "    print(mean_)\n",
    "    B = shapes - mean_.T #centering the data\n",
    "    \n",
    "    # Covariant matrix\n",
    "    #Cov_matrix = np.dot(B.T,B)\n",
    "    Cov_matrix = np.cov(B)\n",
    "   \n",
    "    # Calculate eigenvalues and eigenvectors of covariant matrix)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(Cov_matrix)\n",
    "    \n",
    "     #svd\n",
    "    #U, S, V = np.linalg.svd(Cov_matrix)\n",
    "    \n",
    "    return eigenvalues, eigenvectors, B, mean\n",
    "\n",
    "#print(eigns(shapes,2)) \n",
    "\n",
    "def pca(X,k):\n",
    "    \n",
    "    eigenvalues, eigenvectors, B, mean_of_x = eigns(X,k)\n",
    "    \n",
    "     # Make a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvalues))]\n",
    "\n",
    "    # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "    eig_pairs.sort(key=lambda x: x[0], reverse=True) \n",
    "    \n",
    "    #Reduce dimensionality\n",
    "    pc = np.hstack([eig_pairs[i][1].reshape(146,1) for i in range(k)])\n",
    "   # print('Matrix W:\\n', pc.shape)\n",
    "    \n",
    "    # Principal component mxk dimension \n",
    "    #pc =  eigenvectors[:,0:k]  \n",
    "    \n",
    "    return eigenvalues, eigenvectors, pc, B, mean_of_x, eig_pairs\n",
    "    \n",
    "\n",
    "#print(pca(shapes,2))\n",
    "\n",
    "\n",
    "def feature_space_to_pc(x,k):\n",
    "    va,ve,pc, B, mean_of_x, eig_pairs = pca(x,k) # pc: nxk , B: nxm, mean_of_x: nxm\n",
    "    # B is (x - mean)                \n",
    "    \n",
    "    b =  np.dot(pc.T, B) # kxn * nxm -> kxm\n",
    "                        \n",
    "    return b\n",
    "\n",
    "#print(feature_space_to_pc(shapes,2))    \n",
    "\n",
    "def pc_to_feature_space(x,k):\n",
    "    eigenvalues, eigenvectors, pc, B, mean_of_x, eig_pairs = pca(x,k) # pc: nxk , B: nxm, mean_of_x: nxm\n",
    "    b = feature_space_to_pc(x,k) # kxm\n",
    "                                \n",
    "    \n",
    "    #feature_space\n",
    "    X =  np.dot(pc, b) # nxk * kxm -> nxm\n",
    "    feature_space = X + mean_of_x.T\n",
    "    \n",
    "    return feature_space\n",
    "\n",
    "#pc_to_feature_space(shapes,2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758d6cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating precision\n",
    "\n",
    "As described above, using PCA to transform a sample $x$ to a principal\n",
    "component space and back again likely results in an error $\\epsilon$, called the _reconstruction error_. In this task you will implement a\n",
    "method for calculating this error and use it to test the effect of\n",
    "increasing or decreasing the number of principal components used.\n",
    "\n",
    "When solving regression problems, the error is typically measured as the\n",
    "average distance error, otherwise known as root mean square error\n",
    "(RMSE). This is also used when calculating the construction error. For\n",
    "reference, the RMSE is\n",
    "\n",
    "$$RMSE(x, \\widetilde{x}) = \\sqrt{\\frac{1}{N}\\sum_i (x_i-\\widetilde{x}_i)^2},$$\n",
    "\n",
    "where $x$, $\\widetilde{x}$ are the original and transformed samples\n",
    "respectively and $N$ is the total number of samples $x_i$.\n",
    "\n",
    "Another method for evaluating PCA models is to look at the eigenvalues,\n",
    "where eigenvalue $i$ is denoted $\\lambda^{(i)}$. The eigenvalues explain\n",
    "the variance of each dimension when that data has been transformed by\n",
    "PCA. The sum of all eigenvalues $\\lambda^{(1)}+\\dots+\\lambda^{(n)}$ is\n",
    "equal to the total variance of the data. By comparing all the\n",
    "eigenvalues we can calculate:\n",
    "\n",
    "(1) **Proportional variance:** What proportion of the total variance is\n",
    "explained by a single component. The following formula can be used\n",
    "\n",
    "$$\\frac{\\lambda^{(i)}}{\\lambda^{(1)} + \\dots + \\lambda^{(n)}}$$\n",
    "\n",
    "(2) **Cumulative proportional variance:** What cumulative proportion of\n",
    "the total variance is explained by the first $k$ components.\n",
    "\n",
    "$$\\frac{\\lambda^{(1)} + \\dots + \\lambda^{(k)}}{\\lambda^{(1)} + \\dots + \\lambda^{(n)}}$$\n",
    "\n",
    "\n",
    "1.  **<i class=\"fas fa-code\"></i> Calculate reconstruction error:** Implement a function in your\n",
    "    script that calculates the reconstruction error given a dataset $X$,\n",
    "    principle components $\\Phi$, and a mean vector $\\mu$.\n",
    "\n",
    "2.  **<i class=\"fas fa-code\"></i> Plot reconstruction error:** When constructing $\\Phi$ you may use a single principal component or all of them. Plot the reconstruction error of $\\Phi$ for all possible numbers of principle components. An example is shown in {numref}`fig:plot_var`.\n",
    "\n",
    "3.  **<i class=\"fas fa-code\"></i> Calculate variance:** Create functions that calculate the\n",
    "    _proportional_ and _cumulative proportional_ variance.\n",
    "\n",
    "4.  **<i class=\"fas fa-code\"></i> Plot variance metrics:** Plot both the proportional and cumulative\n",
    "    proportional variance in a single plot. An example is shown in\n",
    "    {numref}`fig:plot_var`.\n",
    "\n",
    "\n",
    "```{figure} ./img/plot_var.png\n",
    "---\n",
    "name: fig:plot_var\n",
    "width: 400px\n",
    "---\n",
    "The expected result of the cumulative and individual variance\n",
    "proportion.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a135de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " ...\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]]\n",
      "[[-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " ...\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]\n",
      " [-0.1233574  -0.10821668 -0.10727042 ...  0.10262503  0.143708\n",
      "   0.04485926]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mpower(X \u001b[38;5;241m-\u001b[39m x, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m120\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rmse\n\u001b[0;32m---> 11\u001b[0m \u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36merror\u001b[0;34m(X, k)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(X,k):\n\u001b[1;32m      5\u001b[0m     \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# returns transformation of feature space to principal component given dataset after calculating  mean and pc\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mpc_to_feature_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mpower(X \u001b[38;5;241m-\u001b[39m x, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m120\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rmse\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mpc_to_feature_space\u001b[0;34m(x, k)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#feature_space\u001b[39;00m\n\u001b[1;32m     86\u001b[0m X \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mdot(pc, b) \u001b[38;5;66;03m# nxk * kxm -> nxm\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m feature_space \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m+\u001b[39m \u001b[43mmean_of_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature_space\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "# 1. Calculate reconstruction error\n",
    "from math import sqrt\n",
    "\n",
    "def error(X,k):\n",
    "    \n",
    "    # returns transformation of feature space to principal component given dataset after calculating  mean and pc\n",
    "    x = pc_to_feature_space(X,k)\n",
    "    rmse = np.sqrt(np.sum(np.power(X - x, 2))/120)\n",
    "    return rmse\n",
    "    \n",
    "error(shapes,2)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your implementation here.\n",
    "\n",
    "def variances(X,k):\n",
    "    eigenvalues, eigenvectors, pc, B, mean_of_x, eig_pairs = pca(X,k) \n",
    "\n",
    "    eig_pairs = np.array(eig_pairs)\n",
    "\n",
    "    sum_of_all_eig_values = 0\n",
    "\n",
    "    for i in eig_pairs:\n",
    "        sum_of_all_eig_values += i[0]\n",
    "\n",
    "    cumulative_variance = []\n",
    "    variance_proportion = []\n",
    "\n",
    "    for i in range(120):\n",
    "        tmp = 0\n",
    "        variance_proportion.append(eig_pairs[i][0]/ sum_of_all_eig_values)\n",
    "\n",
    "        for j in range(i):\n",
    "            tmp += eig_pairs[j][0]\n",
    "\n",
    "        cumulative_variance.append(tmp)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(variance_proportion, '-|', label=\"Variance proportion\")\n",
    "    plt.plot(cumulative_variance, '-|', label=\"Cumulative proportion\")\n",
    "    plt.legend()\n",
    "    \n",
    "print(variances(shapes,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
